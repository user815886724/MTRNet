Namespace(arch='MPTR_SuperviseNet', att_se=False, batch_size=32, checkpoint=50, dataset='SIDD', embed_dim=32, env='_', eval_workers=0, global_skip=False, gpu='0,1', local_skip=False, lr_initial=0.0002, mode='denoising', n_epoch=250, norm_layer='nn.LayerNorm', optimizer='adamw', pretrain_weights='model\\model_best.pth', resume=False, save_dir='model', save_images=False, token_mlp='leff', train_dir='data\\SIDD\\train', train_ps=128, train_workers=0, use_gpu=False, val_dir='data\\SIDD\\val', vit_depth=12, vit_dim=256, vit_mlp_dim=512, vit_nheads=8, vit_patch_size=16, vit_share=False, warmup=False, warmup_epochs=3, weight_decay=0.02, win_size=8)

MPTR_SuperviseNet(
  (shallow_feat): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): CAB(
      (CA): CALayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_du): Sequential(
          (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): Sigmoid()
        )
      )
      (norm): Layer_Norm(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (body): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): PReLU(num_parameters=1)
        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (u_former): U_former(
    embed_dim=32, token_projection=linear, token_mlp=leff,win_size=8
    (pos_drop): Dropout(p=0.0, inplace=False)
    (input_projection): InputProjection(
      (projection): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.01, inplace=True)
      )
    )
    (output_projection): OutputProjection(
      (projection): Sequential(
        (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (encoder_layer0): TransformerBlocks(
      (blocks): ModuleList(
        (0): TransformerVision(
          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=32, out_features=32, bias=True)
              (to_kv): Linear(in_features=32, out_features=64, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=32, out_features=32, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=32, out_features=128, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=128, out_features=32, bias=True)
            )
          )
        )
        (1): TransformerVision(
          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=32, out_features=32, bias=True)
              (to_kv): Linear(in_features=32, out_features=64, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=32, out_features=32, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=32, out_features=128, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=128, out_features=32, bias=True)
            )
          )
        )
      )
    )
    (dowsample_0): DownSample(
      (conv): Sequential(
        (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      )
    )
    (encoder_layer1): TransformerBlocks(
      (blocks): ModuleList(
        (0): TransformerVision(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=64, out_features=64, bias=True)
              (to_kv): Linear(in_features=64, out_features=128, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=64, out_features=64, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=64, out_features=256, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=256, out_features=64, bias=True)
            )
          )
        )
        (1): TransformerVision(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=64, out_features=64, bias=True)
              (to_kv): Linear(in_features=64, out_features=128, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=64, out_features=64, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=64, out_features=256, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=256, out_features=64, bias=True)
            )
          )
        )
      )
    )
    (dowsample_1): DownSample(
      (conv): Sequential(
        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      )
    )
    (encoder_layer2): TransformerBlocks(
      (blocks): ModuleList(
        (0): TransformerVision(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=128, out_features=128, bias=True)
              (to_kv): Linear(in_features=128, out_features=256, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=128, out_features=128, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
            )
          )
        )
        (1): TransformerVision(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=128, out_features=128, bias=True)
              (to_kv): Linear(in_features=128, out_features=256, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=128, out_features=128, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
            )
          )
        )
      )
    )
    (dowsample_2): DownSample(
      (conv): Sequential(
        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      )
    )
    (bottleneck): TransformerBlocks(
      (blocks): ModuleList(
        (0): TransformerVision(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=256, out_features=256, bias=True)
              (to_kv): Linear(in_features=256, out_features=512, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=1024, out_features=256, bias=True)
            )
          )
        )
        (1): TransformerVision(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=256, out_features=256, bias=True)
              (to_kv): Linear(in_features=256, out_features=512, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=1024, out_features=256, bias=True)
            )
          )
        )
      )
    )
    (upsample_0): UpSample(
      (de_conv): Sequential(
        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (decoder_layer0): TransformerBlocks(
      (blocks): ModuleList(
        (0): TransformerVision(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=256, out_features=256, bias=True)
              (to_kv): Linear(in_features=256, out_features=512, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=1024, out_features=256, bias=True)
            )
          )
        )
        (1): TransformerVision(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=256, out_features=256, bias=True)
              (to_kv): Linear(in_features=256, out_features=512, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=256, out_features=256, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=1024, out_features=256, bias=True)
            )
          )
        )
      )
    )
    (upsample_1): UpSample(
      (de_conv): Sequential(
        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (decoder_layer1): TransformerBlocks(
      (blocks): ModuleList(
        (0): TransformerVision(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=128, out_features=128, bias=True)
              (to_kv): Linear(in_features=128, out_features=256, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=128, out_features=128, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
            )
          )
        )
        (1): TransformerVision(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=128, out_features=128, bias=True)
              (to_kv): Linear(in_features=128, out_features=256, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=128, out_features=128, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=512, out_features=128, bias=True)
            )
          )
        )
      )
    )
    (upsample_2): UpSample(
      (de_conv): Sequential(
        (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (decoder_layer2): TransformerBlocks(
      (blocks): ModuleList(
        (0): TransformerVision(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=64, out_features=64, bias=True)
              (to_kv): Linear(in_features=64, out_features=128, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=64, out_features=64, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=64, out_features=256, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=256, out_features=64, bias=True)
            )
          )
        )
        (1): TransformerVision(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attention): WindowAttention(
            (qkv): LinearProjection(
              (to_q): Linear(in_features=64, out_features=64, bias=True)
              (to_kv): Linear(in_features=64, out_features=128, bias=True)
            )
            (attention_drop): Dropout(p=0.0, inplace=False)
            (projection): Linear(in_features=64, out_features=64, bias=True)
            (se_layer): Identity()
            (projection_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): LeFF(
            (linear1): Sequential(
              (0): Linear(in_features=64, out_features=256, bias=True)
              (1): GELU()
            )
            (depth_conv): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
              (1): GELU()
            )
            (linear2): Sequential(
              (0): Linear(in_features=256, out_features=64, bias=True)
            )
          )
        )
      )
    )
    (final_decoder): OutputProjection(
      (projection): Sequential(
        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (csff_encoder0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
    (csff_encoder1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (csff_encoder2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (csff_decoder2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (csff_decoder1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
    (csff_decoder0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (concat): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (input_projection): InputProjection(
    (projection): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (stage_encoder): Encoder(
    (encoder_level1): Sequential(
      (0): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (encoder_level2): Sequential(
      (0): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (encoder_level3): Sequential(
      (0): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (down12): DownSample_S(
      (down): Sequential(
        (0): Upsample(scale_factor=0.5, mode=bilinear)
        (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (down23): DownSample_S(
      (down): Sequential(
        (0): Upsample(scale_factor=0.5, mode=bilinear)
        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (stage_decoder): Decoder(
    (decoder_level1): Sequential(
      (0): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (decoder_level2): Sequential(
      (0): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (decoder_level3): Sequential(
      (0): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (1): CAB(
        (CA): CALayer(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv_du): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): ReLU(inplace=True)
            (2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (3): Sigmoid()
          )
        )
        (norm): Layer_Norm(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (body): Sequential(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): PReLU(num_parameters=1)
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (skip_attn1): CAB(
      (CA): CALayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_du): Sequential(
          (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): Sigmoid()
        )
      )
      (norm): Layer_Norm(
        (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      )
      (body): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): PReLU(num_parameters=1)
        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (skip_attn2): CAB(
      (CA): CALayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (conv_du): Sequential(
          (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): ReLU(inplace=True)
          (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): Sigmoid()
        )
      )
      (norm): Layer_Norm(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (body): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): PReLU(num_parameters=1)
        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (up21): SkipUpSample(
      (up): Sequential(
        (0): Upsample(scale_factor=2.0, mode=bilinear)
        (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
    (up32): SkipUpSample(
      (up): Sequential(
        (0): Upsample(scale_factor=2.0, mode=bilinear)
        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
  (sam): SAM_S(
    (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (conv2): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (conv3): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (norm1): Layer_Norm(
      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
    (norm2): Layer_Norm(
      (norm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)
    )
  )
)

Epoch: 1	Time: 6.9204	Loss: 0.0000	LearningRate 0.000200
Epoch: 2	Time: 4.8346	Loss: 0.0000	LearningRate 0.000200
Epoch: 3	Time: 3.5275	Loss: 0.0000	LearningRate 0.000200
Epoch: 4	Time: 3.1617	Loss: 0.0000	LearningRate 0.000200
Epoch: 5	Time: 8.9958	Loss: 0.0000	LearningRate 0.000200
Epoch: 6	Time: 8.3860	Loss: 0.0000	LearningRate 0.000200
Epoch: 7	Time: 2.8757	Loss: 0.0000	LearningRate 0.000200
Epoch: 8	Time: 5.4819	Loss: 0.0000	LearningRate 0.000200
Epoch: 9	Time: 2.5638	Loss: 0.0000	LearningRate 0.000200
